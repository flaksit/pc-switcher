THESE ARE DRAFT THOUGHTS/NOTES FOR NEW FEATURES. SHOULD NOT BE READ BY AI AGENTS OR INCLUDED IN ANSWERS.


Find all dead code and remove it. E.g. unused imports, unused functions, unused variables, etc.

---

Refactor: Give Context to constructor instead of to each method. This simplifies method signatures and shortens code. E.g. self._log() doesn't need to get the context anymore, as it is already stored in self.context.

---



@docs/adr/considerations/async-vs-sync-concurrency-analysis.md explored and documented in DRAFT some ideas for concurrency.
We chose to use asyncio for concurrency.
Now, I want this decision to be clearly documented, including diagrams. However, what is documented there is not the cleanest, most simple design and possibly doesn't correspond at 100% with the specs from @specs/001-foundation/spec.md .


---



Need to execute package removal as well. Need to be able to select packages that are only on a single host (e.g. nvidia drivers on P17, not on XPS).
--> Keep a list of packages that are system-specific
--> On sync, list newly installed packages (only the ones installed explicitly; not the dependencies) and prompt the user if they are system-specific (so should not be synced) or normal (should be synced). If the target system contains packages that are not in the received sync, ask the same: system-specific packages so should be kept and normal packages should be removed.
We want to work only with "user selected packages" (apt install); not packages that only were installed because they were a dependency.



Update the relevant sections of the relevant .md files with this. Don't change anything else. Respect the current level of detail of each file.



Syncthing for /etc should NOT be running all the time in the background. Because package installs could install new or modify existing files in /etc with the package default values, and syncthing immediately overwriting target -> source, while we want syncthing overwriting source -> target, even if the file is newer on target.

More general: for good observability and consistency, system state and Syncthing syncs should be taken from the snapshots and not from the "live" folders, so that we are sure we can rollback to a consistent state.

Workflow should be
1. take /etc snapshot on both source and target
2. diff system state (this changes files in ~/system-state/)
3. diff /etc of the snapshot (this changes /etc/.stignore)
4. We need a new diff from /etc on source in case /etc/.stignore changed. We could remove the previous snapshot.
5. apply system state (packages)
6. syncthing /etc from the source snapshot

7. take /home snapshot on both source and target (including ~/system-state)
8. syncthing /home from the source snapshot

1 > 2 > [ [ 2 > 3 > 4> [ 5 | 6 ] ] | [ 7 > 8 ] ]

So 7>8 can run in parallel as from 2.
5 and 6 can run in parallel as well.

Or am I overanalysing/overengineering it?




Before you asked following Question 1: "Are you using "Send Only" mode on the source, or "Receive Only" mode on the target?"

Is it possible to have "Send Only" on the source and "Send&Receive" on the target? If so, how will both Scenarios turn out?

Is it possible to have "Send&Receive" on the source and "Receive Only" on the target? If so, how will both Scenarios turn out?



Syncthing for /etc should NOT be running all the time in the background. Reason:
- Package installs could install new files or modify existing files in /etc with the package default values. In that case, syncthing would overwrite target -> source (because the target has the newer files), while we want syncthing overwriting source -> target, even if the file is newer on target.
So Syncthing for /etc should only be started manually when we want to sync /etc from source to target, after taking snapshots on both sides, reviewing the diffs and selecting which /etc we want to sync (always) or not (never).
Syncthing for /etc should run uni-directionally from source to target only. It should override local changes on target (leaving conflict files if any), even if the target mtime is more recent than the source mtime.

Syncthing for /home could run all the time in the background, even bi-directionally. Though this should be an optional step. At least in the beginning, we want to have full control over the sync process, so we want it to be manual and uni-directional from source to target only, just as with /etc.

For good observability and consistency, system state and Syncthing syncs should be taken from btrfs snapshots and not from the "live" folders, so that we are sure we can rollback to a consistent state.

Workflow should be
1. take /etc snapshot on both source and target
2. diff system state (this changes files in ~/system-state/)
3. diff /etc of the snapshot (this changes /etc/.stignore)
4. We need a new diff from /etc on source in case /etc/.stignore changed. We could remove the previous snapshot.
5. apply system state (packages)
6. syncthing /etc from the source snapshot

7. take /home snapshot on both source and target (including ~/system-state)
8. syncthing /home from the source snapshot

1 > 2 > [ [ 2 > 3 > 4> [ 5 | 6 ] ] | [ 7 > 8 ] ]

So 7>8 can run in parallel as from 2.
5 and 6 can run in parallel as well.



--- DONE ---



/speckit.specify Write specs for features 1, 2 and 3 of the @"docs/Feature breakdown.md". This is only a part of the full project requirements as written in @"docs/High level requirement.md".

Notes:
- A first User Story is a specification of what a "module" is and how it integrates into the overall sync process. This specification will need to be so detailed that it can be used to implement all other modules concurrently. It will cover things like configuration, logging, error handling, progress reporting, etc.
- For feature 1, the first thing the sync script should do, is install/upgrade the pc-switcher package on the target machine to the same version as on the source machine.
- For feature 2, a btrfs snapshot should be created on both the source and the target machine before starting the sync and after completing the sync.
- Instead of starting "bare" commands on the target, it is of course possible to wrap them in scripts (that are part of the pc-switcher package) if that makes communication between source and target easier (e.g. progress, status, error reporting, logging). This is no requirement, just a suggestion.
- Configuration is about
  - enabling/disabling the modules
  - module-specific configuration
  - setting log levels for file and cli separately
- Where possible, implement (parts of) feature 2 as a required module/plugin (that can NOT be disabled).
- The user should be able to interrupt the sync at any time (e.g. Ctrl-C) and the system should handle this gracefully.
- Logging should contain six levels: DEBUG, FULL, INFO, WARNING, ERROR, CRITICAL. CRITICAL errors should abort the sync (just like the user interrupt). The INFO log level should be used for high-level operation reporting (e.g. "Starting module X", "Module X completed successfully", etc.). DEBUG log level should be used for detailed operation reporting (e.g. command outputs, progress percentages, etc.). FULL log level shows the detail up to file level: each file copied, each snapshot created, etc. DEBUG adds more details about operations (like verbose command outputs).
- For testing purposes, at least three dummy modules should be specified that simulate long-running operations with progress reporting and logging. They should run dummy operations (sleep/loop) on both source and target machine sequentially
  - A successful, only emitting INFO, WARNING and ERROR logs
  - A module that runs into a CRITICAL error. The module itself would just go on running. This is to test that the overall sync aborts on CRITICAL errors.
  - A module that fails partway through (e.g. raising an exception), without properly logging a CRITICAL error.



---


Background info about the project:
- @docs/High level requirements.md
- Focus only on features 1, 2 and 3 of the @"docs/Feature breakdown.md".
- User specifications are in @specs/001-foundation/spec.md.

We made the following tech decisions:
1. See ADRs: @docs/adr/_index.md (follow links if needed)
2. asyncio for handling concurrency between ui, orchestrator, jobs (parallel and sequential), logging, remote execution over SSH
3. BTRFS Snapshots (pre and post) and the Disk Space Monitor will be all be implemented as Jobs, just like the SyncJobs that the user can configure. The main difference: BtrfsSnapshotsJob and DiskSpaceMonitor are "hardcoded" in the sense that the user cannot configure them: they cannot be disabled and their order in the execution flow is fixed.

You are a software architect. Ultrathink to come up with a clean architecture and design that fulfills the requirements defined in @specs/001-foundation/spec.md. It should be as simple as possible, but not simpler. Do not read other documents of this project! I want you to think this through from a clean slate without being influenced by old docs.
Follow DRY and YAGNI principles and best practices for modern (Python 3.13) software architecture/design/engineering.

Execution flow:
```
1. Validate all jobs (disk_space_monitor, btrfs_snapshot, sync jobs)
2. Start parallel: DiskSpaceMonitorJob.execute() (run until the end)
3. Execute sequential: BtrfsSnapshotJob(phase="pre").execute()
4. Execute sequential: SyncJob1.execute()
5. Execute sequential: SyncJob2.execute()
6. Execute sequential: BtrfsSnapshotJob(phase="post").execute()
7. Stop parallel: DiskSpaceMonitorJob.cancel()
```

Create a markdown document "architecture-gemini-2.5.md" in folder @specs/001-foundation/ with at least the following contents:
- A diagram of the components, with explanation of their scope/responsibilities and their relationships
- A class diagram with explanation of the classes, their scope/responsibilities and their relationships
- A sequence diagram when user aborts (Ctrl+C)
- A sequence diagram when a Job raises an exception (= critical failure)
- A sequence diagram when a Job has a command running on the target machine and that remote command fails somehow
- A sequence diagram of a job that logs a message (should be written to file and filtered to be shown on the UI as well)
- A sequence diagram of a job that logs progress (should be shown in UI)
- A sequence diagram when the DiskSpaceMonitor detects a remote disk becoming too full
- Something to illustrate the (streaming) output to the UI, from multiple sources that run in parallel (orchestrator, jobs, heath status of the SSH connection, etc.)

For the diagrams, each time with a textual explanation of the concepts in the diagram and the links and interactions between them.

Note: Jobs will run commands on the target machine as well and the job needs to get the commands output (e.g. to create log messages and to show progress) and needs to control them (e.g. to terminate them in a controlled manner, with timeouts).

Ask clarifying questions when needed!

---



